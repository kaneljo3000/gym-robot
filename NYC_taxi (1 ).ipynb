{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cac5587-e1f8-4923-a62f-636e0556aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import expr, date_format, when, col, to_timestamp, array, count, first, month, to_date, dayofweek,\\\n",
    "datediff, current_date, date_add, max, year, percent_rank\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder, IndexToString, VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64f0fe6-edb8-4397-bd1b-6376eedc1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecae1d09-7f27-4ea3-928a-f94191fb2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('GCPUnzipExample').getOrCreate()\n",
    "df_taxi = spark.read.option('header', 'true').parquet('gs://msca-bdp-student-gcs/unzipped-taxi-data/nyc-taxi-data/',inferSchema=True)\n",
    "df_csv = spark.read.csv(\"gs://msca-bdp-student-gcs/unzipped-taxi-data/taxi+_zone_lookup.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bbd065-7cfc-4cb1-9885-14f5c1ac5409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f09e917-c54a-4650-966e-5a19fd6313f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2011-01-01 00:10:00|  2011-01-01 00:12:00|              4|          0.0|         1|              null|         145|         145|           1|        2.9|  0.5|    0.5|      0.28|         0.0|                  0.0|        4.18|                null|       null|\n",
      "|       2| 2011-01-01 00:04:00|  2011-01-01 00:13:00|              4|          0.0|         1|              null|         264|         264|           1|        5.7|  0.5|    0.5|      0.24|         0.0|                  0.0|        6.94|                null|       null|\n",
      "|       2| 2011-01-01 00:14:00|  2011-01-01 00:16:00|              4|          0.0|         1|              null|         264|         264|           1|        2.9|  0.5|    0.5|      1.11|         0.0|                  0.0|        5.01|                null|       null|\n",
      "|       2| 2011-01-01 00:04:00|  2011-01-01 00:06:00|              5|          0.0|         1|              null|         146|         146|           1|        2.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.9|                null|       null|\n",
      "|       2| 2011-01-01 00:08:00|  2011-01-01 00:08:00|              5|          0.0|         1|              null|         146|         146|           1|        2.5|  0.5|    0.5|      0.11|         0.0|                  0.0|        3.61|                null|       null|\n",
      "|       2| 2011-01-01 00:23:00|  2011-01-01 00:23:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n",
      "|       2| 2011-01-01 00:25:00|  2011-01-01 00:25:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|\n",
      "|       1| 2011-01-01 00:58:10|  2011-01-01 01:15:35|              1|          8.0|         1|                 N|         138|         256|           2|       20.1|  0.5|    0.5|       0.0|         0.0|                  0.0|        21.1|                null|       null|\n",
      "|       1| 2011-01-01 00:23:27|  2011-01-01 00:39:39|              1|          1.6|         1|                 N|         170|         237|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|\n",
      "|       1| 2011-01-01 00:42:08|  2011-01-01 00:51:50|              4|          2.5|         1|                 N|         237|         170|           2|        8.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.1|                null|       null|\n",
      "|       1| 2011-01-01 00:53:36|  2011-01-01 01:17:43|              2|          3.9|         1|                 N|         170|         239|           1|       14.9|  0.5|    0.5|      2.38|         0.0|                  0.0|       18.28|                null|       null|\n",
      "|       1| 2011-01-01 00:37:47|  2011-01-01 00:41:20|              2|          0.6|         1|                 N|          90|          90|           2|        4.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.1|                null|       null|\n",
      "|       1| 2011-01-01 00:42:49|  2011-01-01 00:52:00|              4|          0.9|         1|                 N|          90|         186|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.5|                null|       null|\n",
      "|       1| 2011-01-01 00:56:28|  2011-01-01 01:22:36|              1|          3.9|         1|                 Y|          90|         238|           2|       15.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        16.3|                null|       null|\n",
      "|       1| 2011-01-01 00:11:22|  2011-01-01 00:14:36|              2|          0.7|         1|                 N|         113|          79|           2|        4.1|  0.0|    0.5|       0.0|         0.0|                  0.0|         4.6|                null|       null|\n",
      "|       1| 2011-01-01 00:16:43|  2011-01-01 00:24:39|              2|          1.9|         1|                 N|          79|         170|           1|        6.9|  0.0|    0.5|       1.0|         0.0|                  0.0|         8.4|                null|       null|\n",
      "|       1| 2011-01-01 00:32:25|  2011-01-01 00:48:46|              2|          3.3|         1|                 N|         170|         142|           2|       11.3|  0.0|    0.5|       0.0|         0.0|                  0.0|        11.8|                null|       null|\n",
      "|       1| 2011-01-01 00:50:52|  2011-01-01 01:25:47|              2|          5.3|         1|                 Y|         142|         112|           2|       20.1|  0.0|    0.5|       0.0|         4.8|                  0.0|        25.4|                null|       null|\n",
      "|       1| 2011-01-01 00:20:22|  2011-01-01 00:26:13|              1|          1.0|         4|                 N|         114|         249|           2|        5.3|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|\n",
      "|       1| 2011-01-01 00:28:45|  2011-01-01 00:46:14|              1|          4.3|         4|                 N|         249|         143|           2|       13.7|  0.5|    0.5|       0.0|         0.0|                  0.0|        14.7|                null|       null|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_taxi.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b97ae-3234-49c9-97a5-48d2e8cda030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_taxi.select(\"tpep_pickup_datetime\",\"tpep_dropoff_datetime\",\"passenger_count\",\"PULocationID\",\"DOLocationID\",\\\n",
    "                    # \"total_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecf1bff-934d-46b3-b7f4-27553faa56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dayofweek, 1:Sunday, 2:Monday, 7:Saturday\n",
    "# weekdays, 1:weekdays, 0:weekends\n",
    "# duration, in use of cleaning data\n",
    "df_taxi_add = (df_taxi.withColumn(\"dayofweek\", dayofweek(\"tpep_pickup_datetime\"))\\\n",
    "               .withColumn(\"pickup_time\", date_format(col(\"tpep_pickup_datetime\"), \"HH:mm:ss\"))\\\n",
    "               .withColumn(\"dropoff_time\", date_format(col(\"tpep_dropoff_datetime\"), \"HH:mm:ss\"))\\\n",
    "               .withColumn(\"month\", month(col(\"tpep_pickup_datetime\")))\\\n",
    "               .withColumn(\"year\", year(\"tpep_pickup_datetime\"))\\\n",
    "               .withColumn(\"weekdays\", when(col(\"dayofweek\").isin(1,7), \"0\").otherwise(\"1\"))\\\n",
    "                .withColumn(\"durations\", F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9578fca-ccdd-42cd-83c2-5e1f5dc935ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+---------+-----------+------------+-----+----+--------+---------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|dayofweek|pickup_time|dropoff_time|month|year|weekdays|durations|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+---------+-----------+------------+-----+----+--------+---------+\n",
      "|       2| 2011-01-01 00:10:00|  2011-01-01 00:12:00|              4|          0.0|         1|              null|         145|         145|           1|        2.9|  0.5|    0.5|      0.28|         0.0|                  0.0|        4.18|                null|       null|        7|   00:10:00|    00:12:00|    1|2011|       0|      120|\n",
      "|       2| 2011-01-01 00:04:00|  2011-01-01 00:13:00|              4|          0.0|         1|              null|         264|         264|           1|        5.7|  0.5|    0.5|      0.24|         0.0|                  0.0|        6.94|                null|       null|        7|   00:04:00|    00:13:00|    1|2011|       0|      540|\n",
      "|       2| 2011-01-01 00:14:00|  2011-01-01 00:16:00|              4|          0.0|         1|              null|         264|         264|           1|        2.9|  0.5|    0.5|      1.11|         0.0|                  0.0|        5.01|                null|       null|        7|   00:14:00|    00:16:00|    1|2011|       0|      120|\n",
      "|       2| 2011-01-01 00:04:00|  2011-01-01 00:06:00|              5|          0.0|         1|              null|         146|         146|           1|        2.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.9|                null|       null|        7|   00:04:00|    00:06:00|    1|2011|       0|      120|\n",
      "|       2| 2011-01-01 00:08:00|  2011-01-01 00:08:00|              5|          0.0|         1|              null|         146|         146|           1|        2.5|  0.5|    0.5|      0.11|         0.0|                  0.0|        3.61|                null|       null|        7|   00:08:00|    00:08:00|    1|2011|       0|        0|\n",
      "|       2| 2011-01-01 00:23:00|  2011-01-01 00:23:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|        7|   00:23:00|    00:23:00|    1|2011|       0|        0|\n",
      "|       2| 2011-01-01 00:25:00|  2011-01-01 00:25:00|              1|          0.0|         1|              null|         146|         146|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.5|                null|       null|        7|   00:25:00|    00:25:00|    1|2011|       0|        0|\n",
      "|       1| 2011-01-01 00:58:10|  2011-01-01 01:15:35|              1|          8.0|         1|                 N|         138|         256|           2|       20.1|  0.5|    0.5|       0.0|         0.0|                  0.0|        21.1|                null|       null|        7|   00:58:10|    01:15:35|    1|2011|       0|     1045|\n",
      "|       1| 2011-01-01 00:23:27|  2011-01-01 00:39:39|              1|          1.6|         1|                 N|         170|         237|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|        7|   00:23:27|    00:39:39|    1|2011|       0|      972|\n",
      "|       1| 2011-01-01 00:42:08|  2011-01-01 00:51:50|              4|          2.5|         1|                 N|         237|         170|           2|        8.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.1|                null|       null|        7|   00:42:08|    00:51:50|    1|2011|       0|      582|\n",
      "|       1| 2011-01-01 00:53:36|  2011-01-01 01:17:43|              2|          3.9|         1|                 N|         170|         239|           1|       14.9|  0.5|    0.5|      2.38|         0.0|                  0.0|       18.28|                null|       null|        7|   00:53:36|    01:17:43|    1|2011|       0|     1447|\n",
      "|       1| 2011-01-01 00:37:47|  2011-01-01 00:41:20|              2|          0.6|         1|                 N|          90|          90|           2|        4.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.1|                null|       null|        7|   00:37:47|    00:41:20|    1|2011|       0|      213|\n",
      "|       1| 2011-01-01 00:42:49|  2011-01-01 00:52:00|              4|          0.9|         1|                 N|          90|         186|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.5|                null|       null|        7|   00:42:49|    00:52:00|    1|2011|       0|      551|\n",
      "|       1| 2011-01-01 00:56:28|  2011-01-01 01:22:36|              1|          3.9|         1|                 Y|          90|         238|           2|       15.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        16.3|                null|       null|        7|   00:56:28|    01:22:36|    1|2011|       0|     1568|\n",
      "|       1| 2011-01-01 00:11:22|  2011-01-01 00:14:36|              2|          0.7|         1|                 N|         113|          79|           2|        4.1|  0.0|    0.5|       0.0|         0.0|                  0.0|         4.6|                null|       null|        7|   00:11:22|    00:14:36|    1|2011|       0|      194|\n",
      "|       1| 2011-01-01 00:16:43|  2011-01-01 00:24:39|              2|          1.9|         1|                 N|          79|         170|           1|        6.9|  0.0|    0.5|       1.0|         0.0|                  0.0|         8.4|                null|       null|        7|   00:16:43|    00:24:39|    1|2011|       0|      476|\n",
      "|       1| 2011-01-01 00:32:25|  2011-01-01 00:48:46|              2|          3.3|         1|                 N|         170|         142|           2|       11.3|  0.0|    0.5|       0.0|         0.0|                  0.0|        11.8|                null|       null|        7|   00:32:25|    00:48:46|    1|2011|       0|      981|\n",
      "|       1| 2011-01-01 00:50:52|  2011-01-01 01:25:47|              2|          5.3|         1|                 Y|         142|         112|           2|       20.1|  0.0|    0.5|       0.0|         4.8|                  0.0|        25.4|                null|       null|        7|   00:50:52|    01:25:47|    1|2011|       0|     2095|\n",
      "|       1| 2011-01-01 00:20:22|  2011-01-01 00:26:13|              1|          1.0|         4|                 N|         114|         249|           2|        5.3|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|        7|   00:20:22|    00:26:13|    1|2011|       0|      351|\n",
      "|       1| 2011-01-01 00:28:45|  2011-01-01 00:46:14|              1|          4.3|         4|                 N|         249|         143|           2|       13.7|  0.5|    0.5|       0.0|         0.0|                  0.0|        14.7|                null|       null|        7|   00:28:45|    00:46:14|    1|2011|       0|     1049|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+---------+-----------+------------+-----+----+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_taxi_add.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e54e92e-780e-446f-81b3-475d99d1f565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 22:15:55 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 79.0 in stage 14.0 (TID 397) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 4): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/11/14 22:16:08 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 79.4 in stage 14.0 (TID 417) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 4): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/11/14 22:16:17 ERROR org.apache.spark.scheduler.TaskSetManager: Task 79 in stage 14.0 failed 10 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o200.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 79 in stage 14.0 failed 10 times, most recent failure: Lost task 79.9 in stage 14.0 (TID 426) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 4): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3709)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2735)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2735)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2942)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:302)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:339)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_taxi_add\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2015\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o200.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 79 in stage 14.0 failed 10 times, most recent failure: Lost task 79.9 in stage 14.0 (TID 426) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 4): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3709)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2735)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2735)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2942)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:302)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:339)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_taxi_add.filter(F.col(\"year\") == 2015).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22323ee4-bd01-4842-959c-c76020e4a42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d6463-bd07-4f10-8688-6060a6a5f56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c39b-218b-4e6f-86ac-a49693479f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c738a3-5a38-4bcd-9d2f-1e46bdea5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1368245364"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_taxi_add.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30531731-f500-4de7-9bf8-dec638315f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b516283-24bf-4245-a352-e63b61169127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e3bce5-c565-483a-abf0-b1871b7442b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning unreasonable data\n",
    "df_taxi_clean = df_taxi_add.filter(((F.col(\"durations\") > 0) & (F.col(\"durations\") < (12 * 3600)))\\\n",
    "                                   & (F.col(\"total_amount\") > 0)\\\n",
    "                                   & ((F.col(\"passenger_count\") > 0) & (F.col(\"passenger_count\") < 7))\\\n",
    "                                   & (F.col(\"PULocationID\").isin(1,263))\\\n",
    "                                   & (F.col(\"DOLocationID\").isin(1,263)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546d4879-938a-4b2d-bfc8-0e18c2c5530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 22:05:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 84.0 in stage 10.0 (TID 245) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 1): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:134)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/11/14 22:05:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 91.2 in stage 10.0 (TID 261) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 1): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n",
      "\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n",
      "\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:134)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/11/14 22:05:47 ERROR org.apache.spark.scheduler.TaskSetManager: Task 84 in stage 10.0 failed 10 times; aborting job\n",
      "23/11/14 22:05:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 92.6 in stage 10.0 (TID 289) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 3): TaskKilled (Stage cancelled)\n",
      "23/11/14 22:05:47 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 93.8 in stage 10.0 (TID 291) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 1): TaskKilled (Stage cancelled)\n",
      "23/11/14 22:05:48 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 91.8 in stage 10.0 (TID 292) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 1): TaskKilled (Stage cancelled)\n",
      "23/11/14 22:05:48 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 94.6 in stage 10.0 (TID 290) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 3): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o195.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 84 in stage 10.0 failed 10 times, most recent failure: Lost task 84.9 in stage 10.0 (TID 287) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 1): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:134)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:134)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_taxi_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:664\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o195.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 84 in stage 10.0 failed 10 times, most recent failure: Lost task 84.9 in stage 10.0 (TID 287) (hub-msca-bdp-dphub-students-haoyu1-w-1.c.msca-bdp-student-ap.internal executor 1): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:134)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainDoubleDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:134)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df_taxi_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "772dcecf-6322-42cc-8d6e-8a4f07d49355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time in seconds and time bins(15 mins per bin)\n",
    "def time_to_seconds(time_col):\n",
    "    return (\n",
    "        expr(\"HOUR(\" + time_col + \")\") * 3600 +\n",
    "        expr(\"MINUTE(\" + time_col + \")\") * 60 +\n",
    "        expr(\"SECOND(\" + time_col + \")\")\n",
    "    )\n",
    "bin_size = 15 * 60 # 15mins\n",
    "df_with_seconds = df_taxi_clean.withColumn(\"pickup_time_in_seconds\", time_to_seconds(\"pickup_time\"))\n",
    "df_with_time_bins = df_with_seconds.withColumn(\"pickup_time_bins\", (col(\"pickup_time_in_seconds\") / bin_size).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6f90d08-b7b9-4b1e-9266-1fa21a22f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+---------+-----------+------------+-----+----+--------+---------+----------------------+----------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|dayofweek|pickup_time|dropoff_time|month|year|weekdays|durations|pickup_time_in_seconds|pickup_time_bins|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+---------+-----------+------------+-----+----+--------+---------+----------------------+----------------+\n",
      "|       2| 2011-01-01 00:10:00|  2011-01-01 00:12:00|              4|          0.0|         1|              null|         145|         145|           1|        2.9|  0.5|    0.5|      0.28|         0.0|                  0.0|        4.18|                null|       null|        7|   00:10:00|    00:12:00|    1|2011|       0|      120|                   600|               0|\n",
      "|       2| 2011-01-01 00:04:00|  2011-01-01 00:13:00|              4|          0.0|         1|              null|         264|         264|           1|        5.7|  0.5|    0.5|      0.24|         0.0|                  0.0|        6.94|                null|       null|        7|   00:04:00|    00:13:00|    1|2011|       0|      540|                   240|               0|\n",
      "|       2| 2011-01-01 00:14:00|  2011-01-01 00:16:00|              4|          0.0|         1|              null|         264|         264|           1|        2.9|  0.5|    0.5|      1.11|         0.0|                  0.0|        5.01|                null|       null|        7|   00:14:00|    00:16:00|    1|2011|       0|      120|                   840|               0|\n",
      "|       2| 2011-01-01 00:04:00|  2011-01-01 00:06:00|              5|          0.0|         1|              null|         146|         146|           1|        2.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.9|                null|       null|        7|   00:04:00|    00:06:00|    1|2011|       0|      120|                   240|               0|\n",
      "|       1| 2011-01-01 00:58:10|  2011-01-01 01:15:35|              1|          8.0|         1|                 N|         138|         256|           2|       20.1|  0.5|    0.5|       0.0|         0.0|                  0.0|        21.1|                null|       null|        7|   00:58:10|    01:15:35|    1|2011|       0|     1045|                  3490|               3|\n",
      "|       1| 2011-01-01 00:23:27|  2011-01-01 00:39:39|              1|          1.6|         1|                 N|         170|         237|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|        7|   00:23:27|    00:39:39|    1|2011|       0|      972|                  1407|               1|\n",
      "|       1| 2011-01-01 00:42:08|  2011-01-01 00:51:50|              4|          2.5|         1|                 N|         237|         170|           2|        8.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.1|                null|       null|        7|   00:42:08|    00:51:50|    1|2011|       0|      582|                  2528|               2|\n",
      "|       1| 2011-01-01 00:53:36|  2011-01-01 01:17:43|              2|          3.9|         1|                 N|         170|         239|           1|       14.9|  0.5|    0.5|      2.38|         0.0|                  0.0|       18.28|                null|       null|        7|   00:53:36|    01:17:43|    1|2011|       0|     1447|                  3216|               3|\n",
      "|       1| 2011-01-01 00:37:47|  2011-01-01 00:41:20|              2|          0.6|         1|                 N|          90|          90|           2|        4.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.1|                null|       null|        7|   00:37:47|    00:41:20|    1|2011|       0|      213|                  2267|               2|\n",
      "|       1| 2011-01-01 00:42:49|  2011-01-01 00:52:00|              4|          0.9|         1|                 N|          90|         186|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.5|                null|       null|        7|   00:42:49|    00:52:00|    1|2011|       0|      551|                  2569|               2|\n",
      "|       1| 2011-01-01 00:56:28|  2011-01-01 01:22:36|              1|          3.9|         1|                 Y|          90|         238|           2|       15.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        16.3|                null|       null|        7|   00:56:28|    01:22:36|    1|2011|       0|     1568|                  3388|               3|\n",
      "|       1| 2011-01-01 00:11:22|  2011-01-01 00:14:36|              2|          0.7|         1|                 N|         113|          79|           2|        4.1|  0.0|    0.5|       0.0|         0.0|                  0.0|         4.6|                null|       null|        7|   00:11:22|    00:14:36|    1|2011|       0|      194|                   682|               0|\n",
      "|       1| 2011-01-01 00:16:43|  2011-01-01 00:24:39|              2|          1.9|         1|                 N|          79|         170|           1|        6.9|  0.0|    0.5|       1.0|         0.0|                  0.0|         8.4|                null|       null|        7|   00:16:43|    00:24:39|    1|2011|       0|      476|                  1003|               1|\n",
      "|       1| 2011-01-01 00:32:25|  2011-01-01 00:48:46|              2|          3.3|         1|                 N|         170|         142|           2|       11.3|  0.0|    0.5|       0.0|         0.0|                  0.0|        11.8|                null|       null|        7|   00:32:25|    00:48:46|    1|2011|       0|      981|                  1945|               2|\n",
      "|       1| 2011-01-01 00:50:52|  2011-01-01 01:25:47|              2|          5.3|         1|                 Y|         142|         112|           2|       20.1|  0.0|    0.5|       0.0|         4.8|                  0.0|        25.4|                null|       null|        7|   00:50:52|    01:25:47|    1|2011|       0|     2095|                  3052|               3|\n",
      "|       1| 2011-01-01 00:20:22|  2011-01-01 00:26:13|              1|          1.0|         4|                 N|         114|         249|           2|        5.3|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|        7|   00:20:22|    00:26:13|    1|2011|       0|      351|                  1222|               1|\n",
      "|       1| 2011-01-01 00:28:45|  2011-01-01 00:46:14|              1|          4.3|         4|                 N|         249|         143|           2|       13.7|  0.5|    0.5|       0.0|         0.0|                  0.0|        14.7|                null|       null|        7|   00:28:45|    00:46:14|    1|2011|       0|     1049|                  1725|               1|\n",
      "|       1| 2011-01-01 00:46:44|  2011-01-01 00:54:28|              1|          1.8|         4|                 N|         143|          24|           1|        7.3|  0.5|    0.5|       2.0|         0.0|                  0.0|        10.3|                null|       null|        7|   00:46:44|    00:54:28|    1|2011|       0|      464|                  2804|               3|\n",
      "|       1| 2011-01-01 00:58:04|  2011-01-01 01:43:22|              1|         17.0|         4|                 N|         166|          89|           2|       41.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        42.3|                null|       null|        7|   00:58:04|    01:43:22|    1|2011|       0|     2718|                  3484|               3|\n",
      "|       1| 2011-01-01 00:16:31|  2011-01-01 00:22:24|              2|          1.3|         1|                 N|         166|         238|           1|        5.7|  0.5|    0.5|       0.3|         0.0|                  0.0|         7.0|                null|       null|        7|   00:16:31|    00:22:24|    1|2011|       0|      353|                   991|               1|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+---------+-----------+------------+-----+----+--------+---------+----------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_time_bins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d17878e-ef10-448e-aacd-b55ee9dd5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------------+----------------+-----------------+\n",
      "|year|month|dayofweek|PULocationID|pickup_time_bins|number_of_pickups|\n",
      "+----+-----+---------+------------+----------------+-----------------+\n",
      "|2011|    1|        7|         106|               3|               16|\n",
      "|2011|    1|        7|         171|               5|                3|\n",
      "|2011|    1|        7|         164|               8|              646|\n",
      "|2011|    1|        7|         237|              14|              100|\n",
      "|2011|    1|        7|         262|              14|               66|\n",
      "|2011|    1|        7|         100|              13|              159|\n",
      "|2011|    1|        7|         141|              18|              181|\n",
      "|2011|    1|        7|         168|              21|                7|\n",
      "|2011|    1|        7|         142|              21|               87|\n",
      "|2011|    1|        7|         262|              22|               43|\n",
      "|2011|    1|        7|          40|              25|                7|\n",
      "|2011|    1|        7|         213|              31|                1|\n",
      "|2011|    1|        7|         249|              33|              164|\n",
      "|2011|    1|        7|         166|              36|              106|\n",
      "|2011|    1|        7|         146|              39|               30|\n",
      "|2011|    1|        7|          82|              43|                8|\n",
      "|2011|    1|        7|          25|              43|               24|\n",
      "|2011|    1|        7|          48|              47|              741|\n",
      "|2011|    1|        7|          25|              47|               18|\n",
      "|2011|    1|        7|         244|              50|               17|\n",
      "+----+-----+---------+------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# add y variable\n",
    "df_with_count = df_with_time_bins.groupby(\"year\",\"month\",\"dayofweek\",\"PULocationID\",\"pickup_time_bins\")\\\n",
    "                    .agg(count(\"*\").alias(\"number_of_pickups\"))\n",
    "df_with_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a3e9bbc-5e08-40df-8c2c-759d2c1fb2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------------+----------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+------------+--------+---------+----------------------+-----------------+\n",
      "|year|month|dayofweek|PULocationID|pickup_time_bins|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|pickup_time|dropoff_time|weekdays|durations|pickup_time_in_seconds|number_of_pickups|\n",
      "+----+-----+---------+------------+----------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+------------+--------+---------+----------------------+-----------------+\n",
      "|2011|    1|        7|         145|               0|       2| 2011-01-01 00:10:00|  2011-01-01 00:12:00|              4|          0.0|         1|              null|         145|           1|        2.9|  0.5|    0.5|      0.28|         0.0|                  0.0|        4.18|                null|       null|   00:10:00|    00:12:00|       0|      120|                   600|               50|\n",
      "|2011|    1|        7|         264|               0|       2| 2011-01-01 00:04:00|  2011-01-01 00:13:00|              4|          0.0|         1|              null|         264|           1|        5.7|  0.5|    0.5|      0.24|         0.0|                  0.0|        6.94|                null|       null|   00:04:00|    00:13:00|       0|      540|                   240|              684|\n",
      "|2011|    1|        7|         264|               0|       2| 2011-01-01 00:14:00|  2011-01-01 00:16:00|              4|          0.0|         1|              null|         264|           1|        2.9|  0.5|    0.5|      1.11|         0.0|                  0.0|        5.01|                null|       null|   00:14:00|    00:16:00|       0|      120|                   840|              684|\n",
      "|2011|    1|        7|         146|               0|       2| 2011-01-01 00:04:00|  2011-01-01 00:06:00|              5|          0.0|         1|              null|         146|           1|        2.9|  0.5|    0.5|       0.0|         0.0|                  0.0|         3.9|                null|       null|   00:04:00|    00:06:00|       0|      120|                   240|               49|\n",
      "|2011|    1|        7|         138|               3|       1| 2011-01-01 00:58:10|  2011-01-01 01:15:35|              1|          8.0|         1|                 N|         256|           2|       20.1|  0.5|    0.5|       0.0|         0.0|                  0.0|        21.1|                null|       null|   00:58:10|    01:15:35|       0|     1045|                  3490|               93|\n",
      "|2011|    1|        7|         170|               1|       1| 2011-01-01 00:23:27|  2011-01-01 00:39:39|              1|          1.6|         1|                 N|         237|           2|        9.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        10.3|                null|       null|   00:23:27|    00:39:39|       0|      972|                  1407|             1056|\n",
      "|2011|    1|        7|         237|               2|       1| 2011-01-01 00:42:08|  2011-01-01 00:51:50|              4|          2.5|         1|                 N|         170|           2|        8.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         9.1|                null|       null|   00:42:08|    00:51:50|       0|      582|                  2528|              495|\n",
      "|2011|    1|        7|         170|               3|       1| 2011-01-01 00:53:36|  2011-01-01 01:17:43|              2|          3.9|         1|                 N|         239|           1|       14.9|  0.5|    0.5|      2.38|         0.0|                  0.0|       18.28|                null|       null|   00:53:36|    01:17:43|       0|     1447|                  3216|              930|\n",
      "|2011|    1|        7|          90|               2|       1| 2011-01-01 00:37:47|  2011-01-01 00:41:20|              2|          0.6|         1|                 N|          90|           2|        4.1|  0.5|    0.5|       0.0|         0.0|                  0.0|         5.1|                null|       null|   00:37:47|    00:41:20|       0|      213|                  2267|              729|\n",
      "|2011|    1|        7|          90|               2|       1| 2011-01-01 00:42:49|  2011-01-01 00:52:00|              4|          0.9|         1|                 N|         186|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.0|         7.5|                null|       null|   00:42:49|    00:52:00|       0|      551|                  2569|              729|\n",
      "|2011|    1|        7|          90|               3|       1| 2011-01-01 00:56:28|  2011-01-01 01:22:36|              1|          3.9|         1|                 Y|         238|           2|       15.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        16.3|                null|       null|   00:56:28|    01:22:36|       0|     1568|                  3388|              698|\n",
      "|2011|    1|        7|         113|               0|       1| 2011-01-01 00:11:22|  2011-01-01 00:14:36|              2|          0.7|         1|                 N|          79|           2|        4.1|  0.0|    0.5|       0.0|         0.0|                  0.0|         4.6|                null|       null|   00:11:22|    00:14:36|       0|      194|                   682|              583|\n",
      "|2011|    1|        7|          79|               1|       1| 2011-01-01 00:16:43|  2011-01-01 00:24:39|              2|          1.9|         1|                 N|         170|           1|        6.9|  0.0|    0.5|       1.0|         0.0|                  0.0|         8.4|                null|       null|   00:16:43|    00:24:39|       0|      476|                  1003|             2358|\n",
      "|2011|    1|        7|         170|               2|       1| 2011-01-01 00:32:25|  2011-01-01 00:48:46|              2|          3.3|         1|                 N|         142|           2|       11.3|  0.0|    0.5|       0.0|         0.0|                  0.0|        11.8|                null|       null|   00:32:25|    00:48:46|       0|      981|                  1945|              996|\n",
      "|2011|    1|        7|         142|               3|       1| 2011-01-01 00:50:52|  2011-01-01 01:25:47|              2|          5.3|         1|                 Y|         112|           2|       20.1|  0.0|    0.5|       0.0|         4.8|                  0.0|        25.4|                null|       null|   00:50:52|    01:25:47|       0|     2095|                  3052|              632|\n",
      "|2011|    1|        7|         114|               1|       1| 2011-01-01 00:20:22|  2011-01-01 00:26:13|              1|          1.0|         4|                 N|         249|           2|        5.3|  0.5|    0.5|       0.0|         0.0|                  0.0|         6.3|                null|       null|   00:20:22|    00:26:13|       0|      351|                  1222|              795|\n",
      "|2011|    1|        7|         249|               1|       1| 2011-01-01 00:28:45|  2011-01-01 00:46:14|              1|          4.3|         4|                 N|         143|           2|       13.7|  0.5|    0.5|       0.0|         0.0|                  0.0|        14.7|                null|       null|   00:28:45|    00:46:14|       0|     1049|                  1725|             1246|\n",
      "|2011|    1|        7|         143|               3|       1| 2011-01-01 00:46:44|  2011-01-01 00:54:28|              1|          1.8|         4|                 N|          24|           1|        7.3|  0.5|    0.5|       2.0|         0.0|                  0.0|        10.3|                null|       null|   00:46:44|    00:54:28|       0|      464|                  2804|              160|\n",
      "|2011|    1|        7|         166|               3|       1| 2011-01-01 00:58:04|  2011-01-01 01:43:22|              1|         17.0|         4|                 N|          89|           2|       41.3|  0.5|    0.5|       0.0|         0.0|                  0.0|        42.3|                null|       null|   00:58:04|    01:43:22|       0|     2718|                  3484|              148|\n",
      "|2011|    1|        7|         166|               1|       1| 2011-01-01 00:16:31|  2011-01-01 00:22:24|              2|          1.3|         1|                 N|         238|           1|        5.7|  0.5|    0.5|       0.3|         0.0|                  0.0|         7.0|                null|       null|   00:16:31|    00:22:24|       0|      353|                   991|              143|\n",
      "+----+-----+---------+------------+----------------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+------------+--------+---------+----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# add y variable to original dataset\n",
    "joined_df_taxi = df_with_time_bins.join(df_with_count, [\"year\",\"month\",\"dayofweek\",\"PULocationID\",\"pickup_time_bins\"],'left')\n",
    "joined_df_taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c5af2fd-aa11-4762-844b-7388cd682fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns that are useful to the model\n",
    "df_nyc_taxi = joined_df_taxi.select(\"year\", \"month\", \"dayofweek\", \"weekdays\", \"PULocationID\", \"pickup_time_bins\",\\\n",
    "                                    \"number_of_pickups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0797ab0d-1d71-4d89-8f62-558a0b1abe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+--------+------------+----------------+-----------------+\n",
      "|year|month|dayofweek|weekdays|PULocationID|pickup_time_bins|number_of_pickups|\n",
      "+----+-----+---------+--------+------------+----------------+-----------------+\n",
      "|2011|    1|        7|       0|         145|               0|               50|\n",
      "|2011|    1|        7|       0|         264|               0|              684|\n",
      "|2011|    1|        7|       0|         264|               0|              684|\n",
      "|2011|    1|        7|       0|         146|               0|               49|\n",
      "|2011|    1|        7|       0|         138|               3|               93|\n",
      "|2011|    1|        7|       0|         170|               1|             1056|\n",
      "|2011|    1|        7|       0|         237|               2|              495|\n",
      "|2011|    1|        7|       0|         170|               3|              930|\n",
      "|2011|    1|        7|       0|          90|               2|              729|\n",
      "|2011|    1|        7|       0|          90|               2|              729|\n",
      "|2011|    1|        7|       0|          90|               3|              698|\n",
      "|2011|    1|        7|       0|         113|               0|              583|\n",
      "|2011|    1|        7|       0|          79|               1|             2358|\n",
      "|2011|    1|        7|       0|         170|               2|              996|\n",
      "|2011|    1|        7|       0|         142|               3|              632|\n",
      "|2011|    1|        7|       0|         114|               1|              795|\n",
      "|2011|    1|        7|       0|         249|               1|             1246|\n",
      "|2011|    1|        7|       0|         143|               3|              160|\n",
      "|2011|    1|        7|       0|         166|               3|              148|\n",
      "|2011|    1|        7|       0|         166|               1|              143|\n",
      "+----+-----+---------+--------+------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nyc_taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4bf52b0-3b43-4cbb-a384-53819c3999aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nyc_taxi.select(\"pickup_time_bins\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5be485af-1845-483c-a913-05b5025636ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nyc_taxi = df_nyc_taxi.withColumn(\"PULocationID\",df_nyc_taxi.PULocationID.cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe363173-417f-4041-a99c-0f652145cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- weekdays: string (nullable = false)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- pickup_time_bins: integer (nullable = true)\n",
      " |-- number_of_pickups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nyc_taxi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c14103c4-7fc3-4e55-9ffa-6fe85863cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nyc_taxi = df_nyc_taxi.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a8627-464b-4716-9d85-2109288f45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:=============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "#convert relevant categorical into one hot encoded\n",
    "indexer1 = StringIndexer(inputCol=\"PULocationID\", outputCol=\"LocationIdx\").setHandleInvalid(\"skip\")\n",
    "indexer2 = StringIndexer(inputCol=\"weekdays\", outputCol=\"WeekdaysIdx\").setHandleInvalid(\"skip\")\n",
    "indexer3 = StringIndexer(inputCol=\"dayofweek\", outputCol=\"DayofweekIdx\").setHandleInvalid(\"skip\")\n",
    "#gather all indexers as inputs to the One Hot Encoder\n",
    "inputs = [indexer1.getOutputCol(),indexer2.getOutputCol(), \\\n",
    "          indexer3.getOutputCol()]\n",
    "\n",
    "#create the one hot encoder\n",
    "encoder = OneHotEncoder(inputCols=inputs,  \\\n",
    "                                 outputCols=[\"LocationVec\",\"WeekdaysVec\",\"DayofweekVec\"])\n",
    "\n",
    "#run it through a pipeline\n",
    "pipeline = Pipeline(stages=[indexer1,indexer2,indexer3,encoder])\n",
    "df_nyc_taxi = pipeline.fit(df_nyc_taxi).transform(df_nyc_taxi)\n",
    "df_nyc_taxi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e1dae-d758-4eed-b352-18c716190c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[ \"year\",\"month\",\"pickup_time_bins\",\n",
    "                                              \"LocationVec\", \"WeekdaysVec\", \"DayofweekVec\"],\n",
    "                                  outputCol=\"features\")\n",
    "df_nyc_taxi = vectorAssembler.transform(df_nyc_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa74fb4-13f2-467f-a142-4eb0e4d3c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets.\n",
    "train_df_taxi, test_df_taxi =  df_nyc_taxi.randomSplit([0.7, 0.3],0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
